#+TITLE: Literate mathematical programming
#+AUTHOR: Parker Knight
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>


*  Linear programming
** Chebyshev center of a polyhedron

Suppose that we are given some polyhedron $P = \{ x \in \mathbb{R}^n : a_i^Tx \leq b_i, i = 1, ... m \} \subset \mathbb{R}^n$.

We are interested in finding the largest Euclidean ball $B = \{x_c + u : ||u||_2 \leq r \}$ that lies in $P$, where $x_c$ is the center of $B$ and $r$ is its radius.

This problem can be rewritten and then solved as a simple linear program. To see this, first suppose that $B$ lies in just one of the halfspaces $a^T_ix \leq b_i$. This means that $a_i^T(x_c + u) \leq b_i$ for all $u$ with $||u||_2 \leq r$. Notice that

$$\sup \{a_i^Tu : ||u||_2 \leq r \} = r||a_i||_2$$

where the supremum is achieved at $u = r\frac{a_i}{||a_i||_2}$. It follows that if $||u||_2 \leq r$, then $a_i^Tx_c + r||a_i||_2 \leq b_i$, which is a linear inequality in $x_c$ and $r$.

Note that $B \subset P$ if and only if this condition holds for $i = 1, ... m$. This yields the following linear program:

$$\begin{align*}
\textrm{maximize    } &r \\
\textrm{s.t.  } &a^T_ix_c + r||a_i||_2 \leq b_i \textrm{,  } i = 1, ... m
\end{align*}$$

A quick example in Julia is given below.


#+begin_src julia
using JuMP, GLPK, LinearAlgebra
a1 = -1;
b1 = 0;
a2 = 2.2;
b2 = 4.5;

model = Model(GLPK.Optimizer);
@variable(model, xc);
@variable(model, 0 <= r);
@constraint(model, hs1, dot(a1, xc) + r * norm(a1) <= b1)
@constraint(model, hs2, dot(a2, xc) + r * norm(a2) <= b2)
@objective(model, Max, r)
optimize!(model)
#+end_src

* Quadratic programming
** Constrained least-squares

We consider the standard least-squares problem $||Ax - b||_2^2$ subject to $l_y \leq x_i \leq u_i$ for $i = 1 ... n$. This is clearly a quadratic program, since least squares is quadratic.

Below is an example of a linear regression analysis on the classic Iris data in which we constrain the coefficients to lie between 0 and 1.

#+begin_src julia
using JuMP, SCS
using RDatasets

iris = dataset("datasets", "iris");
X = iris[:,1:3] |> Matrix
Y = iris[:,4]

model = Model(SCS.Optimizer)
@variable(model, 0 <= b[1:3] <= 1)
@objective(model, Min, (X * b - Y)' * (X * b - Y))
optimize!(model)
#+end_src

#+RESULTS:

** Linear program with random cost

Consider a standard linear problem $c^Tx$ with the usual constraints. Suppose that the cost vector $c$ is random with mean $\bar{c}$ and covariance $\Sigma$. Then, the cost $c^Tx$ is a random scalar with mean $\bar{c}^Tx$ and variance $x^T\Sigma x$ (this is simple to check).

There is a trade-off between having a small expected cost and a small cost variance. To solve the original program while taking variance into account, we minimize a linear combination of the expected cost and its variance, $\bar{c}^Tx + \gamma x^T\Sigma x$, where $\gamma \geq 0$ is called the risk-aversion parameter. This clearly yields a quadratic program in $x$. An example is given below.

#+begin_src julia
using JuMP, SCS

n = 10;
A = rand(n,n)
sigma = A * A'
cbar = randn(n)
gamma = 0.8

model = Model(SCS.Optimizer)
@variable(model, 0 <= x[1:n] <= 1)
@constraint(model, con, sum(x) == 1)
@objective(model, Min, cbar' * x + gamma * x' * sigma * x)
optimize!(model)
#+end_src
